<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>sparagus</title><link>https://example.com/</link><description>Lessons I've learned while doing my side-projects.</description><atom:link href="https://example.com/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020 &lt;a href="mailto:alexanderjuda@gmail.com"&gt;Alex Juda&lt;/a&gt; </copyright><lastBuildDate>Fri, 16 Oct 2020 17:25:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>NLU is just slot filling</title><link>https://example.com/posts/nlu-is-just-slot-filling/</link><dc:creator>Alex Juda</dc:creator><description>&lt;div&gt;&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;This post is a compilation of my ideas about how NLU should look like in a task-oriented dialog system.
Any statement without a link is my opinion.&lt;/p&gt;
&lt;h3&gt;tl;dr&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Natural-language_understanding"&gt;NLU&lt;/a&gt; is actually just slot filling&lt;/li&gt;
&lt;li&gt;use &lt;a href="https://en.wikipedia.org/wiki/Named-entity_recognition"&gt;NER&lt;/a&gt; + grammars and a whole text classifier&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Context&lt;/h2&gt;
&lt;p&gt;Let's say you're building a &lt;a href="https://en.wikipedia.org/wiki/Dialogue_system"&gt;dialog system&lt;/a&gt; to allow customers of some company to get things done without talking to a real human in a call center.
Chances are you're going to use a &lt;a href="https://arxiv.org/pdf/2003.07490.pdf"&gt;task-oriented dialog system&lt;/a&gt; framework to build your software. Its architecture looks like a tuning fork:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://example.com/images/dialog_system_tuning_fork.svg"&gt;&lt;/p&gt;
&lt;p&gt;There are many ways to implement components of such system, e.g. a dialog manager can be &lt;a href="https://web.stanford.edu/~jurafsky/slp3/24.pdf"&gt;rule-based&lt;/a&gt;, &lt;a href="https://lekta.ai/"&gt;graph-based&lt;/a&gt;, or a &lt;a href="https://rasa.com/docs/rasa/stories"&gt;supervised ML model&lt;/a&gt;. 
The component we'll focus on for the sake of this post is NLU; usually, it's a subsystem bounded by having user utterance text as an input (sometimes with additional dialog context information) and producing a &lt;a href="https://hao-fang.github.io/ee596_spr2018/slides/week_2-spoken_language_understanding.pdf"&gt;semantic frame&lt;/a&gt;, structured representation of a user utterance usable by dialog manager to update the dialog state and drive the further conversation.&lt;/p&gt;
&lt;h2&gt;Slots and entities&lt;/h2&gt;
&lt;p&gt;What data exactly and in what shape is contained in a semantic frame differs between the implementations, but it can be generalized to a collection of key-value pairs that I'll call &lt;em&gt;slot fills&lt;/em&gt;. 
An example semantic frame for a first utterance in a flight booking assistant application could look like this:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;nlu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"yo, tell me what flights to Berlin are available next week"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;"courtesy"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"greeting"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"intent"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"search_flights"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"flight_destination_city"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"BER"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"flight_window_start"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"2020-10-12"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"flight_window_end"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"2020-10-19"&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;A &lt;em&gt;slot&lt;/em&gt; can be thought of as an atomic information container. 
A &lt;em&gt;dialog task&lt;/em&gt; (AKA &lt;em&gt;dialog strategy&lt;/em&gt;, &lt;em&gt;dialog skill&lt;/em&gt;) defines the bot's behavior by reacting to slot state changes and requesting slot values from the user in a form of an explicit prompt, or an external system e.g. in a form of HTTP requests.&lt;/p&gt;
&lt;p&gt;Slots can be divided into two groups depending on the nature of the data that they accept:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Non-categorical slots. 
   You need to parse the underlying text to extract the value in a structured format, so you do care where exactly in the utterance this information occurs.
   Usually, it's difficult to enumerate each possible value a non-categorical slot can take.
   Non-categorical slots accept time, date, number, phone number, credit card number, etc.
   In the above snippet, &lt;code&gt;flight_destination&lt;/code&gt;, &lt;code&gt;flight_window_start&lt;/code&gt;, and &lt;code&gt;flight_window_end&lt;/code&gt; are examples of this.
   The task of detecting these is &lt;a href="http://www.iro.umontreal.ca/~lisa/pointeurs/taslp_RNNSLU_final_doubleColumn.pdf"&gt;traditionally called&lt;/a&gt; slot filling, but IMHO it's not enough.&lt;/li&gt;
&lt;li&gt;Categorical slots. 
   These are slots that accept a value from a well-defined set of possible cases (think of an enum).
   You only care &lt;em&gt;what&lt;/em&gt; value occurred, not &lt;em&gt;where&lt;/em&gt; it occurred in the utterance.
   In the above snippet, &lt;code&gt;courtesy&lt;/code&gt; and &lt;code&gt;intent&lt;/code&gt; are examples of this.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Non-categorical slots are similar to &lt;em&gt;entities&lt;/em&gt; in the &lt;a href="https://en.wikipedia.org/wiki/Named-entity_recognition"&gt;NER&lt;/a&gt; problem.
The difference is that NER usually concerns universally accepted entities, like &lt;em&gt;person&lt;/em&gt;, &lt;em&gt;date&lt;/em&gt;, or &lt;em&gt;place&lt;/em&gt;. 
Slots are project-dependent and a single dialog system can contain multiple slots that accept similar, but distinct values, like &lt;em&gt;flight window start&lt;/em&gt; and &lt;em&gt;flight window end&lt;/em&gt;, or &lt;em&gt;flight departure&lt;/em&gt; and &lt;em&gt;flight destination&lt;/em&gt;.
Another variation is that NER returns merely the location of an entity in the utterance and a slot needs a structured value, not a free-form utterance substring.&lt;/p&gt;
&lt;p&gt;While conceptually different, we'll use tools that train NER models to train a token tagger model to recognize project-specific slot locations.&lt;/p&gt;
&lt;h2&gt;Slot filling model&lt;/h2&gt;
&lt;p&gt;The overall pipeline looks like this.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://example.com/images/slot_filling_model_overall.svg"&gt;&lt;/p&gt;
&lt;h3&gt;Span Recognizer&lt;/h3&gt;
&lt;p&gt;It's a box that detects what slots are mentioned in the text and returns the substring positions.
Assuming a standard word tokenization, an example slot recognition can look like this:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# word index     0   1    2  3    4       5  6      7   8         9    10&lt;/span&gt;
&lt;span class="n"&gt;span_recognizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"yo, tell me what flights to Berlin are available next week"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# spans                                      &amp;lt;-A--&amp;gt;               &amp;lt;---B---&amp;gt;&lt;/span&gt;
&lt;span class="c1"&gt;#                                                                 &amp;lt;---C---&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;"flight_destination_city"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;  &lt;span class="c1"&gt;# span A&lt;/span&gt;
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"flight_window_start"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="c1"&gt;# span B&lt;/span&gt;
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"flight_window_end"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]]&lt;/span&gt;   &lt;span class="c1"&gt;# span C&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;If you ever &lt;a href="https://demo.allennlp.org/named-entity-recognition/MjMyNjk4Mg=="&gt;played with a NER model&lt;/a&gt; this should look familiar.&lt;/p&gt;
&lt;p&gt;We'll build Span Recognizer as a supervised token classifier with following approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Conditional Random Field – the way to do NER in classical ML&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fastText embeddings + LSTM – transfer learning to utilize pretrained knowledge about the world + an RNN to look at the token context in the utterance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT or another transformer architecture (TBD)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See next posts for details.&lt;/p&gt;
&lt;h3&gt;Value Extractor&lt;/h3&gt;
&lt;p&gt;Value Extractor extracts structured value from utterance substring returned by Span Recognizer.&lt;/p&gt;
&lt;p&gt;Continuing the example about flight booking:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;value_extractor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"yo, tell me what flights to Berlin are available next week"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"flight_destination_city"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"flight_destination_city"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"BER"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;value_extractor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"yo, tell me what flights to Berlin are available next week"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"flight_window_start"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"flight_window_start"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"2020-10-12"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;value_extractor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"yo, tell me what flights to Berlin are available next week"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"flight_window_end"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"flight_window_end"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"2020-10-19"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Another example in a top up dialog could look like this:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;#                0    1  2 3   4  5   6  7&lt;/span&gt;
&lt;span class="n"&gt;value_extractor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"make me a top up for 20 bucks"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"top_up_value"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"top_up_value"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"20"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;#                0    1  2 3   4  5   6  7&lt;/span&gt;
&lt;span class="n"&gt;value_extractor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"make me a top up for 20 bucks"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"top_up_currency"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"top_up_currency"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"USD"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Like other black boxes in our diagram, Value Extractor is just an &lt;em&gt;interface&lt;/em&gt;. The actual implementation is whatever makes sense for a given project, e.g.:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regex + custom code&lt;/li&gt;
&lt;li&gt;&lt;a href="https://duckling.wit.ai/"&gt;rule-based&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.geonames.org/search.html?q=berlin&amp;amp;country="&gt;Geonames API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Classifier&lt;/h3&gt;
&lt;p&gt;It's a simple &lt;a href="https://medium.com/@MageshDominator/machine-learning-based-multi-label-text-classification-9a0e17f88bb4"&gt;multi label text classifier&lt;/a&gt; with a label mangling post processing.
Text goes in, a list of predicted classes comes out.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"yo, tell me what flights to Berlin are available next week"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;"courtesy"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"greeting"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"intent"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"search_flights"&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"make me a top up for 20 bucks"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;"intent"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"top_up"&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;mangling&lt;/em&gt; can be needed because label encoders in many ML frameworks support labels in a form of simple strings.&lt;/p&gt;
&lt;p&gt;Let's say your dialog system consists of a couple of intents and categorical values. 
All slots values can be enumerated:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;["intent", "top_up"]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;["intent", "search_flights"]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;["courtesy", "greeting"]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;["courtesy", "thank_you"]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;["travel_class", "business"]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;["travel_class", "premium_economy"]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;["travel_class", "economy"]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It can be mangled, so that the classifier's label encoder only sees single strings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;"intent/top_up"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;"intent/search_flights"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;"courtesy/greeting"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;"courtesy/thank_you"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;"travel_class/business"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;"travel_class/premium_economy"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;"travel_class/economy"&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that this is possible for data like intents or categorical variables because all values can be enumerated easily.
It wouldn't be feasible to enumerate all 32-bit integers, even though it's theoretically possible:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;"top_up_value/0"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;"top_up_value/1"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;"top_up_value/2"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;li&gt;&lt;code&gt;"top_up_value/4294967295"&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><guid>https://example.com/posts/nlu-is-just-slot-filling/</guid><pubDate>Mon, 12 Oct 2020 20:08:11 GMT</pubDate></item></channel></rss>